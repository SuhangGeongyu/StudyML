{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.169 Perceptron 를 텐서플로로 표현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.]\n",
      " [4.]]\n",
      "[[1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [7.]]\n"
     ]
    }
   ],
   "source": [
    "X_data = [[0, 0],[0, 1],[1, 0],[1, 1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2])\n",
    "\n",
    "W = tf.Variable([[2.],[4.]], name=\"weight\")\n",
    "b = tf.Variable([1.], name = \"bias\")\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(W))\n",
    "    print(sess.run(hypothesis, feed_dict={X:X_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.172 Perceptron의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[2. 4.]] b: [1.] cost: [[36.]]\n",
      "W: [[0.79999995 2.8       ]] b: [-0.20000005] cost: [[5.7599993]]\n",
      "W: [[0.31999996 2.32      ]] b: [-0.68000007] cost: [[0.9215996]]\n",
      "W: [[0.128 2.128]] b: [-0.87200004] cost: [[0.14745605]]\n",
      "W: [[0.05119999 2.0512    ]] b: [-0.9488] cost: [[0.02359288]]\n",
      "W: [[0.02048005 2.02048   ]] b: [-0.97951996] cost: [[0.00377489]]\n",
      "W: [[0.00819202 2.0081918 ]] b: [-0.991808] cost: [[0.00060398]]\n"
     ]
    }
   ],
   "source": [
    "X_data = [[1, 1]]\n",
    "Y_data = [[1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2])\n",
    "Y = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "W = tf.Variable([[2.],[4.]], name=\"weight\")\n",
    "b = tf.Variable([1.], name = \"bias\")\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.square(hypothesis - Y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(7):\n",
    "        W_val, b_val, cost_val = sess.run([W, b, cost], \n",
    "                                      feed_dict={X:X_data, Y:Y_data})\n",
    "        sess.run(train, feed_dict={X:X_data, Y:Y_data})\n",
    "        print(\"W:\", W_val.reshape(1,2), \"b:\", b_val, \"cost:\", cost_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.175 AND 게이트 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[0.5000007 0.5000007]] b: [-0.25000077] cost: 0.0625 \n",
      " hypothesis:\n",
      " [[-0.25000077]\n",
      " [ 0.24999994]\n",
      " [ 0.24999994]\n",
      " [ 0.75000066]] \n",
      " predicted:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "X_data = [[0, 0],[0, 1],[1, 0],[1, 1]]\n",
    "Y_data = [[0],[0],[0],[1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2])\n",
    "Y = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "W = tf.Variable([[2.],[4.]], name=\"weight\")\n",
    "b = tf.Variable([1.], name = \"bias\")\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X:X_data, Y:Y_data})\n",
    "    W_val, b_val, cost_val, hyp_val, pre_val = sess.run(\n",
    "        [W, b, cost, hypothesis, predicted], feed_dict={X:X_data, Y:Y_data})                                               \n",
    "    print(\"W:\", W_val.reshape(1,2), \"b:\", b_val, \"cost:\", cost_val,\n",
    "          \"\\n hypothesis:\\n\", hyp_val, '\\n predicted:\\n', pre_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.180 시그모이드 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[3.3827615 3.4324124]] b: [-5.3185835] cost: 0.12072968 \n",
      " cost_detail:\n",
      " [[0.0048877 ]\n",
      " [0.1411969 ]\n",
      " [0.13479799]\n",
      " [0.20203611]] \n",
      " hypothesis:\n",
      " [[0.0048758 ]\n",
      " [0.13168165]\n",
      " [0.12610757]\n",
      " [0.8170654 ]] \n",
      " predicted:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "X_data = [[0, 0],[0, 1],[1, 0],[1, 1]]\n",
    "Y_data = [[0],[0],[0],[1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2])\n",
    "Y = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "W = tf.Variable([[2.],[4.]], name=\"weight\")\n",
    "b = tf.Variable([1.], name = \"bias\")\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost_Detail = -Y * tf.log(hypothesis) -(1 - Y) * tf.log(1 - hypothesis)\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X:X_data, Y:Y_data})\n",
    "    W_val, b_val, cost_val, cost_Detail_val, hyp_val, pre_val = sess.run(\n",
    "        [W, b, cost, cost_Detail, hypothesis, predicted], feed_dict={X:X_data, Y:Y_data})                                               \n",
    "    print(\"W:\", W_val.reshape(1,2), \"b:\", b_val, \"cost:\", cost_val, \"\\n cost_detail:\\n\", cost_Detail_val,\n",
    "          \"\\n hypothesis:\\n\", hyp_val, '\\n predicted:\\n', pre_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.182 시그모이드 & MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[2.1486247 2.5496874]] b: [-3.6718142] cost: 0.040638063 \n",
      " hypothesis:\n",
      " [[0.02479963]\n",
      " [0.24561699]\n",
      " [0.17899232]\n",
      " [0.73623633]] \n",
      " predicted:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "X_data = [[0, 0],[0, 1],[1, 0],[1, 1]]\n",
    "Y_data = [[0],[0],[0],[1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2])\n",
    "Y = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "W = tf.Variable([[2.],[4.]], name=\"weight\")\n",
    "b = tf.Variable([1.], name = \"bias\")\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X:X_data, Y:Y_data})\n",
    "    W_val, b_val, cost_val, hyp_val, pre_val = sess.run(\n",
    "        [W, b, cost, hypothesis, predicted], feed_dict={X:X_data, Y:Y_data})                                               \n",
    "    print(\"W:\", W_val.reshape(1,2), \"b:\", b_val, \"cost:\", cost_val,\n",
    "          \"\\n hypothesis:\\n\", hyp_val, '\\n predicted:\\n', pre_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.183 시그모이드 미분그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "def deff_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "x_data = np.linspace(-10, 10, 100)\n",
    "y_data = deff_sigmoid(x_data)\n",
    "\n",
    "plt.plot(x_data, y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.235 Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.234596\n",
      "200 0.6109049\n",
      "400 0.501715\n",
      "600 0.4057542\n",
      "800 0.31276512\n",
      "1000 0.2428631\n",
      "1200 0.21935341\n",
      "1400 0.19997402\n",
      "1600 0.18363053\n",
      "1800 0.16966866\n",
      "2000 0.15761118\n",
      "--------------\n",
      "[[2.326574e-06 8.208166e-04 9.991768e-01]] [2]\n",
      "[[0.00201385 0.08471417 0.913272  ]] [2]\n",
      "[[9.0210442e-08 1.6583623e-01 8.3416361e-01]] [2]\n",
      "--------------\n",
      "[[2.3265741e-06 8.2081661e-04 9.9917680e-01]\n",
      " [2.0138491e-03 8.4714204e-02 9.1327190e-01]\n",
      " [9.0210442e-08 1.6583627e-01 8.3416361e-01]] [2 2 2]\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "            \n",
    "    print('--------------')\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 2, 1, 1]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[2, 1, 3, 2]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[3, 1, 3, 4]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "    print('--------------')\n",
    "    \n",
    "    all = sess.run(hypothesis, feed_dict={\n",
    "                   X: [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4]]})\n",
    "    \n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.348 Learning Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 1 \n",
      "W:\n",
      " [[1. 2.]\n",
      " [3. 4.]] \n",
      "b: [1. 2.] \n",
      "cost: 4.01815 \n",
      "hypothesis:\n",
      " [[0.018 0.982]] \n",
      "predicted:\n",
      " [1]\n",
      "--------------------\n",
      "step : 2 \n",
      "W:\n",
      " [[1.0982 1.9018]\n",
      " [3.1964 3.8036]] \n",
      "b: [1.0982 1.9018] \n",
      "cost: 2.879391 \n",
      "hypothesis:\n",
      " [[0.0562 0.9438]] \n",
      "predicted:\n",
      " [1]\n",
      "--------------------\n",
      "step : 3 \n",
      "W:\n",
      " [[1.1926 1.8074]\n",
      " [3.3852 3.6148]] \n",
      "b: [1.1926 1.8074] \n",
      "cost: 1.8584802 \n",
      "hypothesis:\n",
      " [[0.1559 0.8441]] \n",
      "predicted:\n",
      " [1]\n",
      "--------------------\n",
      "step : 4 \n",
      "W:\n",
      " [[1.277 1.723]\n",
      " [3.554 3.446]] \n",
      "b: [1.277 1.723] \n",
      "cost: 1.0872645 \n",
      "hypothesis:\n",
      " [[0.3371 0.6629]] \n",
      "predicted:\n",
      " [1]\n",
      "--------------------\n",
      "step : 5 \n",
      "W:\n",
      " [[1.3433 1.6567]\n",
      " [3.6866 3.3134]] \n",
      "b: [1.3433 1.6567] \n",
      "cost: 0.63524795 \n",
      "hypothesis:\n",
      " [[0.5298 0.4702]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step : 6 \n",
      "W:\n",
      " [[1.3903 1.6097]\n",
      " [3.7806 3.2194]] \n",
      "b: [1.3903 1.6097] \n",
      "cost: 0.40865976 \n",
      "hypothesis:\n",
      " [[0.6645 0.3355]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step : 7 \n",
      "W:\n",
      " [[1.4238 1.5762]\n",
      " [3.8477 3.1523]] \n",
      "b: [1.4238 1.5762] \n",
      "cost: 0.29081345 \n",
      "hypothesis:\n",
      " [[0.7477 0.2523]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step : 8 \n",
      "W:\n",
      " [[1.4491 1.5509]\n",
      " [3.8982 3.1018]] \n",
      "b: [1.4491 1.5509] \n",
      "cost: 0.22261134 \n",
      "hypothesis:\n",
      " [[0.8004 0.1996]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step : 9 \n",
      "W:\n",
      " [[1.469  1.531 ]\n",
      " [3.9381 3.0619]] \n",
      "b: [1.469 1.531] \n",
      "cost: 0.17917848 \n",
      "hypothesis:\n",
      " [[0.836 0.164]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n",
      "step : 10 \n",
      "W:\n",
      " [[1.4854 1.5146]\n",
      " [3.9709 3.0291]] \n",
      "b: [1.4854 1.5146] \n",
      "cost: 0.14942771 \n",
      "hypothesis:\n",
      " [[0.8612 0.1388]] \n",
      "predicted:\n",
      " [0]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "X_data = [[1, 2]]\n",
    "Y_data = [[1, 0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2])\n",
    "Y = tf.placeholder(tf.float32, [None,2])\n",
    "\n",
    "W = tf.Variable([[1., 2.],[3., 4.]], name=\"weight\")\n",
    "b = tf.Variable([1., 2.], name = \"bias\")\n",
    "                \n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10):\n",
    "        W_val, b_val, cost_val, hyp_val = sess.run(\n",
    "            [W, b, cost, hypothesis], feed_dict={X:X_data, Y:Y_data})\n",
    "        pred = sess.run(tf.argmax(hyp_val, 1))\n",
    "        print('step :', step+1, '\\nW:\\n', np.round(W_val, 4), '\\nb:', np.round(b_val, 4), '\\ncost:', cost_val,\n",
    "              '\\nhypothesis:\\n', np.round(hyp_val, 4),\n",
    "              '\\npredicted:\\n', pred)\n",
    "        print('-'*20)\n",
    "        sess.run(train, feed_dict={X:X_data, Y:Y_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p.370 Batch, Andgate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0 \n",
      "W :\n",
      " [[1. 2.]\n",
      " [3. 4.]] \n",
      "b:\n",
      " [[1. 2.]] \n",
      " hypothesis:\n",
      " [[0.27 0.73]\n",
      " [0.12 0.88]\n",
      " [0.12 0.88]\n",
      " [0.05 0.95]] \n",
      " --------------------\n",
      "step :  1 \n",
      "W :\n",
      " [[1.83 1.17]\n",
      " [3.83 3.17]] \n",
      "b:\n",
      " [[ 3.45 -0.45]] \n",
      " hypothesis:\n",
      " [[0.98 0.02]\n",
      " [0.99 0.01]\n",
      " [0.99 0.01]\n",
      " [0.99 0.01]] \n",
      " --------------------\n",
      "step :  2 \n",
      "W :\n",
      " [[0.85 2.15]\n",
      " [2.85 4.15]] \n",
      "b:\n",
      " [[2.49 0.51]] \n",
      " hypothesis:\n",
      " [[0.88 0.12]\n",
      " [0.66 0.34]\n",
      " [0.66 0.34]\n",
      " [0.35 0.65]] \n",
      " --------------------\n",
      "step :  3 \n",
      "W :\n",
      " [[0.84 2.16]\n",
      " [2.84 4.16]] \n",
      "b:\n",
      " [[2.93 0.07]] \n",
      " hypothesis:\n",
      " [[0.95 0.05]\n",
      " [0.82 0.18]\n",
      " [0.82 0.18]\n",
      " [0.55 0.45]] \n",
      " --------------------\n",
      "step :  4 \n",
      "W :\n",
      " [[0.46 2.54]\n",
      " [2.46 4.54]] \n",
      "b:\n",
      " [[2.79 0.21]] \n",
      " hypothesis:\n",
      " [[0.93 0.07]\n",
      " [0.62 0.38]\n",
      " [0.62 0.38]\n",
      " [0.17 0.83]] \n",
      " --------------------\n"
     ]
    }
   ],
   "source": [
    "X_data = [[0, 0],[0, 1],[1, 0],[1, 1]]\n",
    "Y_data = [[1, 0],[1, 0],[1, 0],[0, 1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2])\n",
    "Y = tf.placeholder(tf.float32, [None,2])\n",
    "\n",
    "nb_classes = 2\n",
    "W = tf.Variable([[1., 2.],[3., 4.]], name=\"weight\")\n",
    "b = tf.Variable([[1., 2.]], name = \"bias\")\n",
    "                \n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "logit = tf.matmul(X, W) + b\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(5):\n",
    "        W_val, b_val, hyp_val = sess.run([W, b, hypothesis], \n",
    "                                         feed_dict={X:X_data, Y:Y_data})\n",
    "        print('step : ', step,\n",
    "              '\\nW :\\n', np.round(W_val, 2), '\\nb:\\n', np.round(b_val, 2),\n",
    "              '\\n hypothesis:\\n', np.round(hyp_val, 2), '\\n', '-'*20)\n",
    "        sess.run(train, feed_dict={X:X_data, Y:Y_data})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TF-CPU)",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
